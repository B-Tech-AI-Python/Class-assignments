{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lab 9:Preprocessing Techniques in NLP Using NLTK package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this lab, we will be exploring how to preprocess tweets for sentiment analysis. We will provide a function for preprocessing tweets during this week's assignment, but it is still good to know what is going on under the hood. By the end of this lab, you will see how to use the [NLTK](http://www.nltk.org) package to perform a preprocessing pipeline for Twitter datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this lab, we will be using the [Natural Language Toolkit (NLTK)](http://www.nltk.org/howto/twitter.html) package, an open-source Python library for natural language processing. It has modules for collecting, handling, and processing Twitter data.\n",
    "\n",
    "For this exercise, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random                              # pseudo-random number generator\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Twitter dataset\n",
    "\n",
    "The sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial. In a local computer however, you can download the data by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads sample twitter dataset. execute the line below if running on a local machine.\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the text fields of the positive and negative tweets by using the module's `strings()` method like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Print the size of positive and negative tweets\n",
    "print('Number of positive tweets: ', None)\n",
    "print('Number of negative tweets: ', None)\n",
    "\n",
    "#TODO:Print the type of positive and negative tweets, using type()\n",
    "print('\\nThe type of all_positive_tweets is: ', None)\n",
    "print('The type of a tweet entry is: ', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is stored in a list and as you might expect, individual tweets are stored as strings.\n",
    "\n",
    "You can make a more visually appealing report by using Matplotlib's [pyplot](https://matplotlib.org/tutorials/introductory/pyplot.html) library. Let us see how to create a [pie chart](https://matplotlib.org/3.2.1/gallery/pie_and_polar_charts/pie_features.html#sphx-glr-gallery-pie-and-polar-charts-pie-features-py) to show the same information as above. This simple snippet will serve you in future visualizations of this kind of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT the positive and negative tweets in a pie-chart\n",
    "# Declare a figure with a custom size\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# labels for the two classes\n",
    "labels = 'Positives', 'Negative'\n",
    "\n",
    "# Sizes for each slide\n",
    "sizes = [len(all_positive_tweets), len(all_negative_tweets)] \n",
    "\n",
    "# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')  \n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at raw texts\n",
    "\n",
    "Before anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we'd like to consider when preprocessing our data.\n",
    "\n",
    "Below, you will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Display a random tweet from positive and negative tweet. The size of positive and negative tweets are 5000 each. \n",
    "#Generate a random number between 0 and 5000 using random.randint()\n",
    "# print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[None])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One observation you may have is the presence of [emoticons](https://en.wikipedia.org/wiki/Emoticon) and URLs in many of the tweets. This info will come in handy in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess raw text for Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "\n",
    "* Tokenizing the string\n",
    "* Lowercasing\n",
    "* Removing stop words and punctuation\n",
    "* Stemming\n",
    "\n",
    "The videos explained each of these steps and why they are important. Let's see how we can do these to a given tweet. We will choose just one and see how this is transformed by each preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our selected sample. Complex enough to exemplify each step\n",
    "tweet = all_positive_tweets[2277]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a few more libraries for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the stopwords from NLTK\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hyperlinks,  Twitter marks and styles\n",
    "\n",
    "Since we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We'll use the [re](https://docs.python.org/3/library/re.html) library to perform regular expression operations on our tweet. We'll define our search pattern and use the `sub()` method to remove matches by substituting with an empty character (i.e. `''`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m' + tweet)\n",
    "print('\\033[94m')\n",
    "\n",
    "# remove old style retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "# remove hyperlinks\n",
    "tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "\n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "print(tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the string\n",
    "\n",
    "To tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The [tokenize](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual) module from NLTK allows us to do these easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('\\033[92m' + tweet2)\n",
    "print('\\033[94m')\n",
    "\n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize tweets\n",
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "print()\n",
    "print('Tokenized string:')\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and punctuations\n",
    "\n",
    "The next step is to remove stop words and punctuation. Stop words are words that don't add significant meaning to the text. You'll see the list provided by NLTK when you run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the english stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "print('Stop words\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the stop words list above contains some words that could be important in some contexts. \n",
    "These could be words like _i, not, between, because, won, against_. You might need to customize the stop words list for some applications. For our exercise, we will use the entire list.\n",
    "\n",
    "For the punctuation, we saw earlier that certain groupings like ':)' and '...'  should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.\n",
    "\n",
    "Time to clean up our tokenized tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweet_tokens)\n",
    "print('\\033[94m')\n",
    "\n",
    "#TODO: Create the empty list to store the clean tweets after removing stopwords and punctuation\n",
    "tweets_clean = None\n",
    "\n",
    "#TODO: Remove stopwords and punctuation from the tweet_tokens\n",
    "for word in None: # Go through every word in your tokens list\n",
    "    if (word not in stopwords_english and  # remove stopwords\n",
    "        word not in string.punctuation):  # remove punctuation\n",
    "        #TODO: Append the clean word in the tweets_clean list\n",
    "        None\n",
    "\n",
    "print('removed stop words and punctuation:')\n",
    "print(tweets_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the words **happy** and **sunny** in this list are correctly spelled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n",
    "\n",
    "Consider the words: \n",
    " * **learn**\n",
    " * **learn**ing\n",
    " * **learn**ed\n",
    " * **learn**t\n",
    " \n",
    "All these words are stemmed from its common root **learn**. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, **happi** and **sunni**. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n",
    "\n",
    " * **happ**y\n",
    " * **happi**ness\n",
    " * **happi**er\n",
    " \n",
    "We can see that the prefix **happi** is more commonly used. We cannot choose **happ** because it is the stem of unrelated words like **happen**.\n",
    " \n",
    "NLTK has different modules for stemming and we will be using the [PorterStemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter) module which uses the [Porter Stemming Algorithm](https://tartarus.org/martin/PorterStemmer/). Let's see how we can use it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweets_clean)\n",
    "print('\\033[94m')\n",
    "\n",
    "# Instantiate stemming class\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "#TODO: Create an empty list to store the stems\n",
    "tweets_stem = None \n",
    "\n",
    "#TODO: Itearate over the tweets_clean fot stemming\n",
    "for word in None:\n",
    "    #TODO:call the stem function for stemming the word\n",
    "    stem_word = None               # stemming word\n",
    "    #TODO:Append the stem_word in tweets_stem list\n",
    "    None\n",
    "\n",
    "print('stemmed words:')\n",
    "print(tweets_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now we have a set of words we can feed into to the next stage of our machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_tweet()\n",
    "\n",
    "As shown above, preprocessing consists of multiple steps before you arrive at the final list of words. We will not ask you to replicate these however. You will use the function `process_tweet(tweet)` available below. \n",
    "\n",
    "To obtain the same result as in the previous code cells, you will only need to call the function `process_tweet()`. Let's do that in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the same tweet\n",
    "tweet = all_positive_tweets[2277]\n",
    "\n",
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweet)\n",
    "print('\\033[94m')\n",
    "\n",
    "#TODO: call the process_tweet function\n",
    "tweets_stem = None\n",
    "\n",
    "print('preprocessed tweet:')\n",
    "print(tweets_stem) # Print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Visualizing word frequencies\n",
    "\n",
    "\n",
    "In this lab, we will focus on the `build_freqs()` helper function and visualizing a dataset fed into it. In our goal of tweet sentiment analysis, this function will build a dictionary where we can lookup how many times a word appears in the lists of positive or negative tweets. This will be very helpful when extracting the features of the dataset in the week's programming assignment. Let's see how this function is implemented under the hood in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Concatenate the lists, 1st part is the positive tweets followed by the negative\n",
    "tweets = None\n",
    "\n",
    "# let's see how many tweets we have\n",
    "print(\"Number of tweets: \", len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will build a labels array that matches the sentiments of our tweets.  This data type works pretty much like a regular list but is optimized for computations and manipulation. The `labels` array will be composed of 10000 elements. The first 5000 will be filled with `1` labels denoting positive sentiments, and the next 5000 will be `0` labels denoting the opposite. We can do this easily with a series of operations provided by the `numpy` library:\n",
    "\n",
    "* `np.ones()` - create an array of 1's\n",
    "* `np.zeros()` - create an array of 0's\n",
    "* `np.append()` - concatenate arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a numpy array representing labels of the tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries\n",
    "\n",
    "In Python, a dictionary is a mutable and indexed collection. It stores items as key-value pairs and uses [hash tables](https://en.wikipedia.org/wiki/Hash_table) underneath to allow practically constant time lookups. In NLP, dictionaries are essential because it enables fast retrieval of items or containment checks even with thousands of entries in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "A dictionary in Python is declared using curly brackets. Look at the next example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'key1': 1, 'key2': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The former line defines a dictionary with two entries. Keys and values can be almost any type ([with a few restriction on keys](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)), and in this case, we used strings. We can also use floats, integers, tuples, etc.\n",
    "\n",
    "### Adding or editing entries\n",
    "\n",
    "New entries can be inserted into dictionaries using square brackets. If the dictionary already contains the specified key, its value is overwritten.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new entry\n",
    "dictionary['key3'] = -5\n",
    "\n",
    "# Overwrite the value of key1\n",
    "dictionary['key1'] = 0\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing values and lookup keys\n",
    "\n",
    "Performing dictionary lookups and retrieval are common tasks in NLP. There are two ways to do this: \n",
    "\n",
    "* Using square bracket notation: This form is allowed if the lookup key is in the dictionary. It produces an error otherwise.\n",
    "* Using the [get()](https://docs.python.org/3/library/stdtypes.html#dict.get) method: This allows us to set a default value if the dictionary key does not exist. \n",
    "\n",
    "Let us see these in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square bracket lookup when the key exist\n",
    "print(dictionary['key2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the key is missing, the operation produce an erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of this line is intended to produce a KeyError\n",
    "print(dictionary['key8'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a square bracket lookup, it is common to use an if-else block to check for containment first (with the keyword `in`) before getting the item. On the other hand, you can use the `.get()` method if you want to set a default value when the key is not found. Let's compare these in the cells below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This prints a value\n",
    "if 'key1' in dictionary:\n",
    "    print(\"item found: \", dictionary['key1'])\n",
    "else:\n",
    "    print('key1 is not defined')\n",
    "\n",
    "# Same as what you get with get\n",
    "print(\"item found: \", dictionary.get('key1', -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prints a message because the key is not found\n",
    "if 'key7' in dictionary:\n",
    "    print(dictionary['key7'])\n",
    "else:\n",
    "    print('key does not exist!')\n",
    "\n",
    "# This prints -1 because the key is not found and we set the default to -1\n",
    "print(dictionary.get('key7', -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the building blocks, let's finally take a look at the **build_freqs()** function below. This is the function that creates the dictionary containing the word counts from each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    #TODO:Create the empty dictionary\n",
    "    freqs = None\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        #TODO: Iterate over all the words returned by calling the process_tweet() function for each tweet\n",
    "        for word in None:\n",
    "            pair = (word, y)\n",
    "            #TODO: If pair matches in the dictionary, then increment the count of corresponding pair.\n",
    "            if pair in freqs:\n",
    "                freqs[pair] =None\n",
    "            else:\n",
    "                #TODO: If pair does not matches in the dictionary, then set the count of corresponding pair as 1.\n",
    "                freqs[pair] = None\n",
    "\n",
    "    #TODO: Return the dictionary\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Call the build_freqs function to create frequency dictionary based on tweets and labels\n",
    "freqs = None\n",
    "\n",
    "#TODO: Display the data type of freqs\n",
    "print(f'type(freqs) = {None}')\n",
    "\n",
    "#TODO: Display the length of the dictionary\n",
    "print(f'len(freqs) = {None}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: prinf all the key-value pair of frequency dictionary\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this does not help much to understand the data. It would be better to visualize this output to gain better insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select a set of words that we would like to visualize. It is better to store this temporary information in a table that is very easy to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\n",
    "keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n",
    "        '‚ù§', ':)', ':(', 'üòí', 'üò¨', 'üòÑ', 'üòç', '‚ôõ',\n",
    "        'song', 'idea', 'power', 'play', 'magnific']\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Create the empty list as list representing our table of word counts, where \n",
    "#each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>].\n",
    "data = None\n",
    "\n",
    "#TODO:Iterate over each word in keys\n",
    "for word in None:\n",
    "    \n",
    "    # initialize positive and negative counts\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    # retrieve number of positive counts\n",
    "    if (word, 1) in freqs:\n",
    "        pos = freqs[(word, 1)]\n",
    "        \n",
    "    # retrieve number of negative counts\n",
    "    if (word, 0) in freqs:\n",
    "        neg = freqs[(word, 0)]\n",
    "        \n",
    "    # append the word counts to the table\n",
    "    data.append([word, pos, neg])\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use a scatter plot to inspect this table visually. Instead of plotting the raw counts, we will plot it in the logarithmic scale to take into account the wide discrepancies between the raw counts (e.g. `:)` has 3568 counts in the positive while only 2 in the negative). The red line marks the boundary between positive and negative areas. Words close to the red line can be classified as neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n",
    "x = np.log([x[1] + 1 for x in data])  \n",
    "\n",
    "# do the same for the negative counts\n",
    "y = np.log([x[2] + 1 for x in data]) \n",
    "\n",
    "# Plot a dot for each pair of words\n",
    "ax.scatter(x, y)  \n",
    "\n",
    "# assign axis labels\n",
    "plt.xlabel(\"Log Positive count\")\n",
    "plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "# Add the word as the label at the same position as you added the points just before\n",
    "for i in range(0, len(data)):\n",
    "    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
    "\n",
    "ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart is straightforward to interpret. It shows that emoticons `:)` and `:(` are very important for sentiment analysis. Thus, we should not let preprocessing steps get rid of these symbols!\n",
    "\n",
    "Furthermore, what is the meaning of the crown symbol? It seems to be very negative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a features for the twitter dataset and Apply any machine learning algorithm to classify the tweets and check the accuracy of your model. Try with Logistic Regression for Classification "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
